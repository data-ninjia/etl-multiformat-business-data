# ETL Pipeline for Multi-Format Business Data

## Overview
This project demonstrates a simple ETL pipeline that processes business data from multiple source formats and loads it into a PostgreSQL database.  
The goal of the project is to simulate a realistic data engineering task: ingest heterogeneous data, validate it, apply basic transformations, and store it in a structured form for further analytics.

---

## Business Context
The pipeline simulates data ingestion from different parts of a small business system:
- orders generated by sales systems  
- customer data coming from external services  
- supplier reference data  
- event or operational data stored in columnar format  

In real-world scenarios, such data often arrives in different formats and needs to be standardized before it can be used for reporting or analysis.

---

## Data Sources
The project works with the following input formats:
- **CSV** — transactional data (e.g. orders)
- **JSON** — semi-structured data (e.g. customers)
- **XML** — reference or master data (e.g. suppliers)
- **Parquet** — analytical or event-style data

All source files are stored locally and treated as external data inputs.

---

## Data Flow
The ETL process follows these steps:
1. Read data from multiple source formats  
2. Validate basic data quality (schema, null values, duplicates)  
3. Apply simple transformations and normalization  
4. Load the processed data into PostgreSQL tables  

Each step is implemented explicitly to keep the pipeline easy to understand and extend.

---

## Project Structure
- `data/` — raw input data in different formats  
- `src/` — ETL logic (extract, transform, load steps)  
- `sql/` — SQL scripts for table creation  
- `docker-compose.yml` — local environment setup  
- `requirements.txt` — Python dependencies  

---

## How to Run the Project
1. Clone the repository  
2. Start the environment using Docker Compose  
3. The ETL pipeline will run and load data into PostgreSQL  
4. Connect to PostgreSQL to verify the loaded tables  

All services run locally and require no cloud resources.

---

## Output
After a successful run:
- PostgreSQL contains separate tables for each business entity  
- Data is stored in a structured and queryable format  
- Tables are ready for analytical queries or further processing  

This setup reflects a typical staging or warehouse loading step in a data pipeline.

---

## Tech Stack
- **Python** — ETL orchestration and logic  
- **Pandas** — data transformation and validation  
- **PostgreSQL** — target database  
- **Docker & Docker Compose** — reproducible local environment  

---

## Notes
This project focuses on clarity and fundamentals rather than performance or scalability.  
It is designed to show understanding of core data engineering concepts such as ETL, data validation, and structured storage.
